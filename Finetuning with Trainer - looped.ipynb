{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7Rwla6Ky6iUA"
   },
   "source": [
    "# Finetuning with Trainer and text-classification model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544715,
     "status": "ok",
     "timestamp": 1712223783678,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "kTnSLtc16iUE",
    "outputId": "a50dcaf4-d038-46e9-a607-73102b1b945f",
    "ExecuteTime": {
     "end_time": "2024-04-06T12:28:57.439061Z",
     "start_time": "2024-04-06T12:28:57.435861Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   colab = True\n",
    "else:\n",
    "   colab = False\n",
    "\n",
    "if colab:\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "    !pip install datasets\n",
    "    !pip install torchinfo\n",
    "    #ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F7z05bbBe5G9",
    "ExecuteTime": {
     "end_time": "2024-04-06T12:28:57.445858Z",
     "start_time": "2024-04-06T12:28:57.439061Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import numbers\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments, DataCollatorWithPadding, AutoTokenizer, pipeline\n",
    "from transformers.modelcard import parse_log_history"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare parameters for each run of finetnuning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "xmkxi8ZMdO9U"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ],
   "metadata": {
    "id": "vURNs2uSdO9U",
    "ExecuteTime": {
     "end_time": "2024-04-06T12:28:57.455869Z",
     "start_time": "2024-04-06T12:28:57.453442Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'roberta-base', 'num_train_epochs': 1, 'save_strategy': 'no', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'split': (90, 10)}\n"
     ]
    }
   ],
   "source": [
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# tested = 'distilbert-base-cased', 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch), 'HuggingFaceH4/tiny-random-LlamaForSequenceClassification'\n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# model_path = 'SamLowe/roberta-base-go_emotions'\n",
    "# model_path = 'distilbert-base-uncased'\n",
    "# model_path = 'xlmoberta'\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103\n",
    "\n",
    "\n",
    "#if list of objects is provided list of all combinations of parameters will be created for running\n",
    "params_tested = {'model': ['roberta-base'],\n",
    "                 'num_train_epochs': 1,\n",
    "                 'save_strategy': 'no', # 'epoch'/'no'\n",
    "                 'per_device_train_batch_size': 8,\n",
    "                 'per_device_eval_batch_size': 64,\n",
    "                 'split': (90, 10) #may be tuple of len 2/3, dividing respectively into 2:{train, test} and 3:{train, test, validate}. By default labels are stratified. You may pass \"balanced\" instead of the last element, number of labels for each class will be     \n",
    "                 }\n",
    "\n",
    "save_logs = False\n",
    "\n",
    "def radio_split_tuple():\n",
    "    split_s = sum([i for i in params_tested['split'] if isinstance(i, numbers.Integral)])\n",
    "    params_tested['split'] = tuple([i/split_s if isinstance(i, numbers.Integral) else i for i in params_tested['split']])\n",
    "     \n",
    "\n",
    "\n",
    "params_tested={i:[q] if type(q) is not list else q for (i,q) in params_tested.items()}\n",
    "keys = list(params_tested.keys())\n",
    "combinations = list(itertools.product(*params_tested.values()))\n",
    "result = [{keys[i]: combination[i] for i in range(len(keys))} for combination in combinations]\n",
    "print(*result, sep='\\n')\n",
    "\n",
    "train_params_looped = result"
   ],
   "metadata": {
    "id": "oylcN846dO9V",
    "outputId": "79bedc27-b958-4529-83e3-9f112e0f732b",
    "ExecuteTime": {
     "end_time": "2024-04-06T12:28:57.463353Z",
     "start_time": "2024-04-06T12:28:57.455869Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 5420,
     "status": "ok",
     "timestamp": 1711981638003,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "NmTIZtvK6iUF",
    "outputId": "9ff554fa-71dd-4258-fac5-23ee1f950e71",
    "ExecuteTime": {
     "end_time": "2024-04-06T12:29:54.773839Z",
     "start_time": "2024-04-06T12:29:54.765512Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_ds(df, train_params_instance):\n",
    "    temp_split = train_params_instance['split']\n",
    "    \n",
    "    target_map = {k:i for i,k in enumerate(df['label'].unique())}\n",
    "    reversed_target_map = {v:k for k, v in target_map.items()}\n",
    "    \n",
    "    df['label'] = df['label'].map(target_map)\n",
    "    \n",
    "    if temp_split[-1]=='balanced' or len(temp_split)==2:\n",
    "        if temp_split[-1]=='balanced':\n",
    "            split = [pd.Series(name='sentence'), pd.Series(name='sentence'), pd.Series(name='label'), pd.Series(name='label')]\n",
    "            n_for_training = df.groupby('label').count()['Source'].min()\n",
    "            train_percentage = df.groupby('label').count()['Source'].apply(lambda x: (temp_split[0]*n_for_training)/x).to_dict()\n",
    "            \n",
    "            for label_name, group in df.groupby('label'):\n",
    "                split_t = train_test_split(group['sentence'], group['label'], test_size=1-train_percentage[label_name], random_state=42, shuffle=True)\n",
    "                split = [pd.concat([i[0],i[1]], axis=0) for i in zip(split, split_t)]\n",
    "        else:\n",
    "            split = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=temp_split[1], random_state=42, shuffle=True)\n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(pd.concat([split[0], split[2]], axis=1))\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([split[1], split[3]], axis=1))\n",
    "    elif len(temp_split)==3:\n",
    "        non_train_size = 1-temp_split[0]\n",
    "        split = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=non_train_size, random_state=42, shuffle=True)\n",
    "        split2 = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=temp_split[2]/non_train_size, random_state=42, shuffle=True)\n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(pd.concat([split[0], split[2]], axis=1))\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([split2[0], split2[2]], axis=1))\n",
    "        ds['validate'] = Dataset.from_pandas(pd.concat([split2[1], split2[3]], axis=1))\n",
    "    return ds, target_map, reversed_target_map\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  acc = np.mean(predictions == labels)\n",
    "  f1 = f1_score(labels, predictions, average='macro')\n",
    "  return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "def save_logs_from_training_run(trainer, train_params, timestamp, trained_model_path, colab, target_map):\n",
    "    log_history = parse_log_history(trainer.state.log_history)\n",
    "    log_df = pd.DataFrame(log_history[1])\n",
    "    log_df.insert(0, 'model', train_params['model'])\n",
    "    log_df.insert(0, 'timestamp', timestamp)\n",
    "    log_df['model_path'] = trained_model_path\n",
    "    log_df['samples_per_s'] = log_history[0]['train_samples_per_second']\n",
    "    log_df['steps_per_s'] = log_history[0]['train_steps_per_second']\n",
    "    log_df['colab'] = colab\n",
    "    log_df['per_device_train_batch_size'] = train_params['per_device_train_batch_size']\n",
    "    log_df['per_device_eval_batch_size'] = train_params['per_device_eval_batch_size']\n",
    "    log_df['split'] = str(train_params['split'])\n",
    "    log_df['target_map'] = str(target_map)\n",
    "\n",
    "    float_cols = log_df.select_dtypes(include='float64')\n",
    "    log_df[float_cols.columns] = float_cols.apply(lambda x: round(x, 3))\n",
    "    log_df.to_csv('output/training_logs.csv', mode='a', header= not os.path.isfile('output/training_logs.csv'), index=False)\n",
    "    \n",
    "def finetune(train_params, ds, target_map, save_logs):\n",
    "    params_passed = {k: train_params[k] for k in train_params if k not in ['model', 'split']}\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    trained_model_path = f'output/models/{train_params['model']}_{timestamp}'\n",
    "    try:\n",
    "        def tokenize_fn(batch):\n",
    "            return tokenizer(batch['sentence'], truncation=True)\n",
    "        \n",
    "        #Tokenize dataset\n",
    "        tokenizer = AutoTokenizer.from_pretrained(train_params['model'], trust_remote_code=True)\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        tokenized_datasets = ds.map(tokenize_fn, batched=True)\n",
    "        \n",
    "        #Change labels\n",
    "        config = AutoConfig.from_pretrained(train_params['model'], trust_remote_code=True)\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "        config.id2label = reversed_target_map\n",
    "        config.label2id = target_map\n",
    "        \n",
    "        #Load model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            train_params['model'], config=config, ignore_mismatched_sizes=True, trust_remote_code=True)\n",
    "    \n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=f'{trained_model_path}/checkpoints',\n",
    "          evaluation_strategy='epoch',\n",
    "          logging_strategy='epoch',\n",
    "          use_cpu = False,\n",
    "          **params_passed\n",
    "        )\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        if save_logs:\n",
    "            save_logs_from_training_run(trainer, train_params, timestamp, trained_model_path, colab, target_map)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        if save_logs:\n",
    "            err_df = pd.DataFrame([[trained_model_path, str(e)]])\n",
    "            err_df.to_csv('output/error_logs.csv', mode='a', index=False)\n",
    "        \n",
    "        \n",
    "def to_binary_classification(x):\n",
    "    if x in ['Positive', 'Negative']:\n",
    "        return 'Pathos'\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ldKE0vAo6iUF"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load PolarIs-Pathos to df\n",
    "# source: https://github.com/kaatkaa/PolarIs-Corpora/blob/main/PolarIs-Full-Corpus/PolarIs-Full-Corpus-Pathos/PolarIs-Pathos.xlsx\n",
    "if colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  data_path = 'drive/MyDrive/master_lm/PolarIs-Pathos.xlsx'\n",
    "else:\n",
    "  data_path = 'data/PolarIs-Pathos.xlsx'\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "df['label'] = df[['No_pathos', 'Positive', 'Negative']].idxmax(axis=1)\n",
    "df['label'] = df['label'].apply(lambda x: to_binary_classification(x))\n",
    "df = df.rename(columns={'Sentence':'sentence'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T12:29:56.726912Z",
     "start_time": "2024-04-06T12:29:55.870400Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/15578 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "686d700620224f1f832ba0ed967da1d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "601991cf93124f51bf224f938b268fe4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1948' max='1948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1948/1948 01:26, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.605900</td>\n      <td>0.502758</td>\n      <td>0.800000</td>\n      <td>0.687500</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run looped \n",
    "for train_params in train_params_looped[:1]:\n",
    "    ds, target_map, reversed_target_map, = df_to_ds(df, train_params)    \n",
    "    finetune(train_params, ds, target_map,save_logs)"
   ],
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "5c2bb70a4b064df2a5d3ecaf631c4336",
      "3834bd538ee340dbb3cef3ff7d0bf354"
     ]
    },
    "id": "bbjzSUKgdO9W",
    "outputId": "a779fa55-9a12-49f2-d30e-b6aee5877358",
    "ExecuteTime": {
     "end_time": "2024-04-06T12:31:25.231714Z",
     "start_time": "2024-04-06T12:29:57.296659Z"
    }
   },
   "execution_count": 32
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
