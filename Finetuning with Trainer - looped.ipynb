{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7Rwla6Ky6iUA"
   },
   "source": "# Finetuning with Trainer and text-classification model validation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544715,
     "status": "ok",
     "timestamp": 1712223783678,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "kTnSLtc16iUE",
    "outputId": "a50dcaf4-d038-46e9-a607-73102b1b945f",
    "ExecuteTime": {
     "end_time": "2024-04-13T17:54:30.178676Z",
     "start_time": "2024-04-13T17:54:30.175196Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   colab = True\n",
    "else:\n",
    "   colab = False\n",
    "\n",
    "if colab:\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "    !pip install datasets\n",
    "    !pip install torchinfo\n",
    "    #ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F7z05bbBe5G9",
    "ExecuteTime": {
     "end_time": "2024-04-13T17:54:30.212196Z",
     "start_time": "2024-04-13T17:54:30.209203Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import numbers\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments, DataCollatorWithPadding, AutoTokenizer, pipeline\n",
    "from transformers.modelcard import parse_log_history"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare parameters for each run of finetuning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "xmkxi8ZMdO9U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# tested = 'distilbert-base-cased', 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch), \n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# model_path = 'SamLowe/roberta-base-go_emotions'\n",
    "# model_path = 'distilbert-base-uncased'\n",
    "# model_path = 'xlmoberta'\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103"
   ],
   "metadata": {
    "id": "vURNs2uSdO9U",
    "ExecuteTime": {
     "end_time": "2024-04-13T17:54:30.218772Z",
     "start_time": "2024-04-13T17:54:30.212196Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "#if list of objects is provided list of all combinations of parameters will be created for running\n",
    "params_tested = {'model': ['michellejieli/emotion_text_classifier'], # Pre-trained model names from the Hugging Face hub used for fine-tuning\n",
    "                 'num_train_epochs': 10, # Number of times the model sees the entire training dataset.\n",
    "                 'save_strategy': 'epoch', # Controls when to save model checkpoints ('epoch' or 'no').\n",
    "                 'per_device_train_batch_size': 8, # Number of samples processed in each training step (personally, 8/16 work best, but 16 may find linear drop in inference speed during fine-tuning).\n",
    "                 'per_device_eval_batch_size': 64, # Number of samples processed in each evaluation step. \n",
    "                 'split': [(80, 10, 10)], # Divides the dataset into training, testing, (and optionally) validation sets. Use 'balanced' for equal class representation in the validation set. Examples: (90,10) -> split into train and test proportionally; (80, 10, 10) splits into train,test, validate proportionally. \n",
    "                 'binary': False, # Indicates whether the task is binary (two classes) or multi-class classification.,\n",
    "                 'balanced': False, # his way labels used for training are split evenly, fitting size to the lowest label count. In (80, 10, 'balanced') n (equal to 80% of least represented label) will be taken from each label, rest will be used for test.\n",
    "                 }\n",
    "\n",
    "# Controls whether to save logs during a process. When set to False, logging is disabled.\n",
    "save_logs = True\n",
    "\n",
    "def ratio_split_tuple(split):\n",
    "    split_s = sum([i for i in split if isinstance(i, numbers.Integral)])\n",
    "    new_split = tuple([i/split_s if isinstance(i, numbers.Integral) else i for i in split])\n",
    "    return new_split\n",
    "\n",
    "if isinstance(params_tested['split'], list):\n",
    "    params_tested['split'] = list(map(ratio_split_tuple, params_tested['split']))\n",
    "else:\n",
    "    params_tested['split'] = ratio_split_tuple(params_tested['split'])\n",
    "\n",
    "params_tested={i:[q] if type(q) is not list else q for (i,q) in params_tested.items()}\n",
    "keys = list(params_tested.keys())\n",
    "combinations = list(itertools.product(*params_tested.values()))\n",
    "result = [{keys[i]: combination[i] for i in range(len(keys))} for combination in combinations]\n",
    "print(*result, sep='\\n')\n",
    "\n",
    "train_params_looped = result"
   ],
   "metadata": {
    "id": "oylcN846dO9V",
    "outputId": "79bedc27-b958-4529-83e3-9f112e0f732b",
    "ExecuteTime": {
     "end_time": "2024-04-13T18:05:33.771083Z",
     "start_time": "2024-04-13T18:05:33.766239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'michellejieli/emotion_text_classifier', 'num_train_epochs': 10, 'save_strategy': 'epoch', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'split': (0.8, 0.1, 0.1), 'binary': False, 'balanced': False}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 5420,
     "status": "ok",
     "timestamp": 1711981638003,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "NmTIZtvK6iUF",
    "outputId": "9ff554fa-71dd-4258-fac5-23ee1f950e71",
    "ExecuteTime": {
     "end_time": "2024-04-13T18:05:34.697057Z",
     "start_time": "2024-04-13T18:05:34.684789Z"
    }
   },
   "source": [
    "# Prepare functions\n",
    "\n",
    "def csv_to_ds(path, binary, split, balanced=False):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, processes the data, and transforms it into a dataset ready for next steps.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to the CSV file.\n",
    "        binary (bool): Flag indicating whether to convert labels to binary classification.\n",
    "        split (float): See explanation and examples for params_tested dict\n",
    "        balanced (bool): Flag indicating whether to balance training dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            * dataset: The processed dataset suitable for machine learning.\n",
    "            * tg_map: A dictionary mapping target labels to numerical values.\n",
    "            * reversed_tg_map: A dictionary mapping numerical values back to original labels. \n",
    "    \"\"\"\n",
    "    # csv to df\n",
    "    new_df = pd.read_excel(path)\n",
    "    new_df['label'] = new_df[['No_pathos', 'Positive', 'Negative']].idxmax(axis=1)\n",
    "    new_df = new_df.rename(columns={'Sentence':'sentence'})\n",
    "    if binary:\n",
    "        new_df['label'] = new_df['label'].apply(lambda x: to_binary_classification(x))\n",
    "\n",
    "    # target map and reversed target map\n",
    "    tg_map = {k:i for i,k in enumerate(new_df['label'].unique())}\n",
    "    reversed_tg_map = {v:k for k, v in tg_map.items()}\n",
    "        \n",
    "    ds = df_to_ds(new_df, split, tg_map, balanced=balanced)\n",
    "    return ds, tg_map, reversed_tg_map\n",
    "\n",
    "def to_binary_classification(x, convert_dict={\"Positive\":'Pathos', 'Negative':'Pathos'}):\n",
    "    if x in convert_dict.keys():\n",
    "        return convert_dict[x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def df_to_ds(df, split, tg_map, balanced=False):    \n",
    "    df['label'] = df['label'].map(tg_map)\n",
    "    if balanced:\n",
    "        train_test = [pd.Series(name='sentence'), pd.Series(name='sentence'), pd.Series(name='label'), pd.Series(name='label')]\n",
    "        n_for_training = df.groupby('label').count()['Source'].min()\n",
    "        train_percentage = df.groupby('label').count()['Source'].apply(lambda x: (split[0]*n_for_training)/x).to_dict()\n",
    "\n",
    "        for label_name, group in df.groupby('label'):\n",
    "            train_test_group = train_test_split(group['sentence'], group['label'], test_size=1-train_percentage[label_name], random_state=42, shuffle=True)\n",
    "            train_test = [pd.concat([i[0],i[1]], axis=0) for i in zip(train_test, train_test_group)]\n",
    "    else:\n",
    "        train_test = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=split[1], random_state=42, shuffle=True)\n",
    "\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(pd.concat([train_test[0], train_test[2]], axis=1))     \n",
    "    if len(split)==2:\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([train_test[1], train_test[3]], axis=1))\n",
    "    elif len(split)==3:\n",
    "        val_ratio = split[2]/(split[1]+split[2])\n",
    "        test_validate = train_test_split(train_test[1], train_test[3], stratify=train_test[3], test_size=val_ratio, random_state=42, shuffle=True)\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([test_validate[0], test_validate[2]], axis=1))\n",
    "        ds['validate'] = Dataset.from_pandas(pd.concat([test_validate[1], test_validate[3]], axis=1))\n",
    "        \n",
    "    return ds\n",
    "\n",
    "# Train functions\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  acc = np.mean(predictions == labels)\n",
    "  f1 = f1_score(labels, predictions, average='macro')\n",
    "  return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "def save_logs_from_training_run(trainer, params, timestamp, trained_model_path, colab, target_map):\n",
    "    log_history = parse_log_history(trainer.state.log_history)\n",
    "    log_df = pd.DataFrame(log_history[1])\n",
    "    log_df.insert(0, 'model', params['model'])\n",
    "    log_df.insert(0, 'timestamp', timestamp)\n",
    "    log_df['model_path'] = trained_model_path\n",
    "    log_df['samples_per_s'] = log_history[0]['train_samples_per_second']\n",
    "    log_df['steps_per_s'] = log_history[0]['train_steps_per_second']\n",
    "    log_df['colab'] = colab\n",
    "    log_df['per_device_train_batch_size'] = params['per_device_train_batch_size']\n",
    "    log_df['per_device_eval_batch_size'] = params['per_device_eval_batch_size']\n",
    "    log_df['split'] = str(params['split'])\n",
    "    log_df['target_map'] = str(target_map)\n",
    "    log_df['binary'] = params['binary']\n",
    "    log_df['balanced'] = params['balanced']\n",
    "\n",
    "    float_cols = log_df.select_dtypes(include='float64')\n",
    "    log_df[float_cols.columns] = float_cols.apply(lambda x: round(x, 3))\n",
    "    log_df.to_csv('output/training_logs.csv', mode='a', header= not os.path.isfile('output/training_logs.csv'), index=False)\n",
    "    \n",
    "def finetune(params, ds, target_map, save_logs):\n",
    "    params_passed = {k: params[k] for k in params if k not in ['model', 'split', 'binary', 'balanced']}\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    trained_model_path = f'output/models/{params['model']}_{timestamp}'\n",
    "    try:\n",
    "        def tokenize_fn(batch):\n",
    "            return tokenizer(batch['sentence'], truncation=True)\n",
    "        \n",
    "        #Tokenize dataset\n",
    "        tokenizer = AutoTokenizer.from_pretrained(params['model'], trust_remote_code=True)\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            # model.resize_token_embeddings(len(tokenizer))\n",
    "        tokenized_datasets = ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "        # data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        \n",
    "        #Change labels\n",
    "        config = AutoConfig.from_pretrained(params['model'], trust_remote_code=True)\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "        config.id2label = reversed_target_map\n",
    "        config.label2id = target_map\n",
    "        \n",
    "        #Load model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            params['model'], config=config, ignore_mismatched_sizes=True, trust_remote_code=True)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=f'{trained_model_path}/checkpoints',\n",
    "          evaluation_strategy='epoch',\n",
    "          logging_strategy='epoch',\n",
    "          use_cpu = False,\n",
    "          **params_passed\n",
    "        )\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            # data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        if save_logs:\n",
    "            save_logs_from_training_run(trainer, params, timestamp, trained_model_path, colab, target_map)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        if save_logs:\n",
    "            err_df = pd.DataFrame([[trained_model_path, str(e)]])\n",
    "            err_df.to_csv('output/error_logs.csv', mode='a', header= not os.path.isfile('output/error_logs.csv'), index=False) \n",
    "    \n",
    "# Validate functions\n",
    "\n",
    "def get_log_for_val(checkpoint_path, sort_col='Step', logs_path='output/training_logs.csv'):\n",
    "    training_logs = pd.read_csv(logs_path)\n",
    "    if 'checkpoint-' in checkpoint_path:\n",
    "        temp_path = checkpoint_path.rsplit('models/', 1)[-1]\n",
    "        model_path, checkpoint_num = temp_path.rsplit('/checkpoints/checkpoint-')\n",
    "        row = training_logs[(training_logs['model_path'].apply(lambda x: x.rsplit('models/', 1)[-1] == model_path)) & (training_logs['Step'] == int(checkpoint_num))]\n",
    "    else:\n",
    "        row = training_logs[training_logs['model_path'] == checkpoint_path].sort_values(sort_col, ascending=False).head(1)\n",
    "    return row.iloc[0]\n",
    "\n",
    "def validate(row, ds):\n",
    "    val_sentences = ds['validate']['sentence']\n",
    "    val_labels = [reversed_target_map[i] for i in ds['validate']['label']]\n",
    "\n",
    "    classifier = pipeline('text-classification', model=os.path.join(row['model_path'], 'checkpoints', f\"checkpoint-{row['Step']}\"), device=0)\n",
    "    predicted = [i['label'] for i in classifier(val_sentences)]\n",
    "    return predicted, val_labels\n",
    "\n",
    "def val_metrics(predicted, val_labels, target_map):\n",
    "    print(\"acc:\", accuracy_score(val_labels, predicted))\n",
    "    print(\"f1:\", f1_score(val_labels, predicted, average='macro'))\n",
    "\n",
    "    cm = confusion_matrix(val_labels, predicted, normalize='true')\n",
    "    plot_cm(cm, target_map)\n",
    "\n",
    "def plot_cm(cm, target_map):\n",
    "    classes = list(target_map.keys())\n",
    "    df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    ax = sn.heatmap(df_cm, annot=True, fmt='.2g')\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Target\")\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ldKE0vAo6iUF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load PolarIs-Pathos to df\n",
    "# source: https://github.com/kaatkaa/PolarIs-Corpora/blob/main/PolarIs-Full-Corpus/PolarIs-Full-Corpus-Pathos/PolarIs-Pathos.xlsx\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_path = 'drive/MyDrive/master_lm/PolarIs-Pathos.xlsx'\n",
    "else:\n",
    "    data_path = 'data/PolarIs-Pathos.xlsx'\n",
    "\n",
    "#run looped \n",
    "for train_params in train_params_looped:\n",
    "    dataset, target_map, reversed_target_map = csv_to_ds(data_path, train_params['binary'], train_params['split'], train_params['balanced'])\n",
    "    finetune(train_params, dataset, target_map, save_logs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T18:14:34.658253Z",
     "start_time": "2024-04-13T18:05:35.613342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14029 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52d66e64f63d4b28b5e865b64b4ecbf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/779 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "595c930f50814a2ba07ea53b995d5629"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/780 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "348d0b788f0d483492bbd2aa65427dbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at michellejieli/emotion_text_classifier and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17540' max='17540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17540/17540 08:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.702500</td>\n",
       "      <td>0.684278</td>\n",
       "      <td>0.698331</td>\n",
       "      <td>0.425636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>0.778936</td>\n",
       "      <td>0.708601</td>\n",
       "      <td>0.483498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.476300</td>\n",
       "      <td>0.747401</td>\n",
       "      <td>0.708601</td>\n",
       "      <td>0.529617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>1.171566</td>\n",
       "      <td>0.715019</td>\n",
       "      <td>0.545047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>1.342070</td>\n",
       "      <td>0.703466</td>\n",
       "      <td>0.510271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>1.612710</td>\n",
       "      <td>0.712452</td>\n",
       "      <td>0.546028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>1.797994</td>\n",
       "      <td>0.695764</td>\n",
       "      <td>0.533153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>2.198389</td>\n",
       "      <td>0.695764</td>\n",
       "      <td>0.543264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>2.334074</td>\n",
       "      <td>0.690629</td>\n",
       "      <td>0.508328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>2.437505</td>\n",
       "      <td>0.708601</td>\n",
       "      <td>0.523251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Validate"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T17:47:33.059793Z",
     "start_time": "2024-04-13T17:47:32.167654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load PolarIs-Pathos to df\n",
    "# # source: https://github.com/kaatkaa/PolarIs-Corpora/blob/main/PolarIs-Full-Corpus/PolarIs-Full-Corpus-Pathos/PolarIs-Pathos.xlsx\n",
    "# if colab:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/drive')\n",
    "#     data_path = 'drive/MyDrive/master_lm/PolarIs-Pathos.xlsx'\n",
    "# else:\n",
    "#     data_path = 'data/PolarIs-Pathos.xlsx'\n",
    "# \n",
    "# df = pd.read_excel(data_path)\n",
    "# df['label'] = df[['No_pathos', 'Positive', 'Negative']].idxmax(axis=1)\n",
    "# df = df.rename(columns={'Sentence':'sentence'})\n",
    "# \n",
    "# checkpoint_path = 'output/models/michellejieli/emotion_text_classifier_2024-04-11_09-02/checkpoints/checkpoint-15590'\n",
    "# # checkpoint_path = 'output/models/michellejieli/emotion_text_classifier_2024-04-11_09-02'    \n",
    "# \n",
    "# val_row = get_log_for_val(checkpoint_path)\n",
    "# val_row['split'] = tuple([float(i) for i in val_row['split'][1:-1].split(', ')])\n",
    "# \n",
    "# dataset, target_map, reversed_target_map = csv_to_ds(data_path, val_row['binary'], val_row['split'])\n",
    "# predicted_labels, true_labels = validate(val_row, dataset)\n",
    "# val_metrics(predicted_labels, true_labels, target_map)\n",
    "# \n",
    "# \n",
    "# print(f'No_pathos: {len([i for i in predicted_labels if i=='No_pathos'])}/{len([i for i in true_labels if i=='No_pathos'])}')\n",
    "# print(f'Negative: {len([i for i in predicted_labels if i=='Negative'])}/{len([i for i in true_labels if i=='Negative'])}')\n",
    "# print(f'Positive: {len([i for i in predicted_labels if i=='Positive'])}/{len([i for i in true_labels if i=='Positive'])}')"
   ],
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 17 fields in line 502, saw 18\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput/models/michellejieli/emotion_text_classifier_2024-04-11_09-02/checkpoints/checkpoint-15590\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# checkpoint_path = 'output/models/michellejieli/emotion_text_classifier_2024-04-11_09-02'    \u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m val_row \u001B[38;5;241m=\u001B[39m \u001B[43mget_log_for_val\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m val_row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m([\u001B[38;5;28mfloat\u001B[39m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m val_row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m)])\n\u001B[0;32m     20\u001B[0m dataset, target_map, reversed_target_map \u001B[38;5;241m=\u001B[39m csv_to_ds(data_path, val_row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m], val_row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[1;32mIn[5], line 156\u001B[0m, in \u001B[0;36mget_log_for_val\u001B[1;34m(checkpoint_path, sort_col, logs_path)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_log_for_val\u001B[39m(checkpoint_path, sort_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStep\u001B[39m\u001B[38;5;124m'\u001B[39m, logs_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput/training_logs.csv\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 156\u001B[0m     training_logs \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcheckpoint-\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m checkpoint_path:\n\u001B[0;32m    158\u001B[0m         temp_path \u001B[38;5;241m=\u001B[39m checkpoint_path\u001B[38;5;241m.\u001B[39mrsplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2061\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 17 fields in line 502, saw 18\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'No_pathos: {len([i for i in predicted if i=='No_pathos'])}/{len([i for i in true_labels if i=='No_pathos'])}')\n",
    "print(f'Negative: {len([i for i in predicted if i=='Negative'])}/{len([i for i in true_labels if i=='Negative'])}')\n",
    "print(f'Positive: {len([i for i in predicted if i=='Positive'])}/{len([i for i in true_labels if i=='Positive'])}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
