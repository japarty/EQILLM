{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7Rwla6Ky6iUA"
   },
   "source": [
    "# Finetuning with Trainer and text-classification model validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544715,
     "status": "ok",
     "timestamp": 1712223783678,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "kTnSLtc16iUE",
    "outputId": "a50dcaf4-d038-46e9-a607-73102b1b945f",
    "ExecuteTime": {
     "end_time": "2024-04-08T09:49:16.665314Z",
     "start_time": "2024-04-08T09:49:16.662071Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   colab = True\n",
    "else:\n",
    "   colab = False\n",
    "\n",
    "if colab:\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "    !pip install datasets\n",
    "    !pip install torchinfo\n",
    "    #ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F7z05bbBe5G9",
    "ExecuteTime": {
     "end_time": "2024-04-08T09:49:16.672086Z",
     "start_time": "2024-04-08T09:49:16.665314Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import numbers\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments, DataCollatorWithPadding, AutoTokenizer, pipeline\n",
    "from transformers.modelcard import parse_log_history"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare parameters for each run of finetnuning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "xmkxi8ZMdO9U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ],
   "metadata": {
    "id": "vURNs2uSdO9U",
    "ExecuteTime": {
     "end_time": "2024-04-08T09:49:16.678908Z",
     "start_time": "2024-04-08T09:49:16.672086Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# tested = 'distilbert-base-cased', 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch), 'HuggingFaceH4/tiny-random-LlamaForSequenceClassification'\n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# model_path = 'SamLowe/roberta-base-go_emotions'\n",
    "# model_path = 'distilbert-base-uncased'\n",
    "# model_path = 'xlmoberta'\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103\n",
    "\n",
    "\n",
    "#if list of objects is provided list of all combinations of parameters will be created for running\n",
    "params_tested = {'model': ['cardiffnlp/twitter-roberta-base-sentiment-latest'],\n",
    "                 'num_train_epochs': 3,\n",
    "                 'save_strategy': 'no', # 'epoch'/'no'\n",
    "                 'per_device_train_batch_size': 8,\n",
    "                 'per_device_eval_batch_size': 64,\n",
    "                 'split': [(90, 10), (90, 10, 'balanced')], #may be tuple of len 2/3, dividing respectively into 2:{train, test} and 3:{train, test, validate}. By default labels are stratified. You may pass \"balanced\" instead of the last element, number of labels for each class will be   \n",
    "                 'binary': True\n",
    "                 }\n",
    "\n",
    "save_logs = True\n",
    "\n",
    "def ratio_split_tuple(split):\n",
    "    split_s = sum([i for i in split if isinstance(i, numbers.Integral)])\n",
    "    new_split = tuple([i/split_s if isinstance(i, numbers.Integral) else i for i in split])\n",
    "    return new_split\n",
    "\n",
    "if isinstance(params_tested['split'], list):\n",
    "    params_tested['split'] = list(map(ratio_split_tuple, params_tested['split']))\n",
    "else:\n",
    "    params_tested['split'] = ratio_split_tuple(params_tested['split'])\n",
    "\n",
    "params_tested={i:[q] if type(q) is not list else q for (i,q) in params_tested.items()}\n",
    "keys = list(params_tested.keys())\n",
    "combinations = list(itertools.product(*params_tested.values()))\n",
    "result = [{keys[i]: combination[i] for i in range(len(keys))} for combination in combinations]\n",
    "print(*result, sep='\\n')\n",
    "\n",
    "train_params_looped = result"
   ],
   "metadata": {
    "id": "oylcN846dO9V",
    "outputId": "79bedc27-b958-4529-83e3-9f112e0f732b",
    "ExecuteTime": {
     "end_time": "2024-04-08T10:32:09.888094Z",
     "start_time": "2024-04-08T10:32:09.883248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'num_train_epochs': 3, 'save_strategy': 'no', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'split': (0.9, 0.1), 'binary': True}\n",
      "{'model': 'cardiffnlp/twitter-roberta-base-sentiment-latest', 'num_train_epochs': 3, 'save_strategy': 'no', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'split': (0.9, 0.1, 'balanced'), 'binary': True}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 5420,
     "status": "ok",
     "timestamp": 1711981638003,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "NmTIZtvK6iUF",
    "outputId": "9ff554fa-71dd-4258-fac5-23ee1f950e71",
    "ExecuteTime": {
     "end_time": "2024-04-08T10:32:10.601991Z",
     "start_time": "2024-04-08T10:32:10.592589Z"
    }
   },
   "source": [
    "def df_to_ds(df, train_params_instance):\n",
    "    temp_split = train_params_instance['split']\n",
    "    \n",
    "    target_map = {k:i for i,k in enumerate(df['label'].unique())}\n",
    "    reversed_target_map = {v:k for k, v in target_map.items()}\n",
    "    \n",
    "    df['label'] = df['label'].map(target_map)\n",
    "    \n",
    "    if temp_split[-1]=='balanced' or len(temp_split)==2:\n",
    "        if temp_split[-1]=='balanced':\n",
    "            split = [pd.Series(name='sentence'), pd.Series(name='sentence'), pd.Series(name='label'), pd.Series(name='label')]\n",
    "            n_for_training = df.groupby('label').count()['Source'].min()\n",
    "            train_percentage = df.groupby('label').count()['Source'].apply(lambda x: (temp_split[0]*n_for_training)/x).to_dict()\n",
    "            \n",
    "            for label_name, group in df.groupby('label'):\n",
    "                split_t = train_test_split(group['sentence'], group['label'], test_size=1-train_percentage[label_name], random_state=42, shuffle=True)\n",
    "                split = [pd.concat([i[0],i[1]], axis=0) for i in zip(split, split_t)]\n",
    "        else:\n",
    "            split = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=temp_split[1], random_state=42, shuffle=True)\n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(pd.concat([split[0], split[2]], axis=1))\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([split[1], split[3]], axis=1))\n",
    "    elif len(temp_split)==3:\n",
    "        non_train_size = 1-temp_split[0]\n",
    "        split = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=non_train_size, random_state=42, shuffle=True)\n",
    "        split2 = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=temp_split[2]/non_train_size, random_state=42, shuffle=True)\n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(pd.concat([split[0], split[2]], axis=1))\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([split2[0], split2[2]], axis=1))\n",
    "        ds['validate'] = Dataset.from_pandas(pd.concat([split2[1], split2[3]], axis=1))\n",
    "    return ds, target_map, reversed_target_map\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  acc = np.mean(predictions == labels)\n",
    "  f1 = f1_score(labels, predictions, average='macro')\n",
    "  return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "def save_logs_from_training_run(trainer, train_params, timestamp, trained_model_path, colab, target_map):\n",
    "    log_history = parse_log_history(trainer.state.log_history)\n",
    "    log_df = pd.DataFrame(log_history[1])\n",
    "    log_df.insert(0, 'model', train_params['model'])\n",
    "    log_df.insert(0, 'timestamp', timestamp)\n",
    "    log_df['model_path'] = trained_model_path\n",
    "    log_df['samples_per_s'] = log_history[0]['train_samples_per_second']\n",
    "    log_df['steps_per_s'] = log_history[0]['train_steps_per_second']\n",
    "    log_df['colab'] = colab\n",
    "    log_df['per_device_train_batch_size'] = train_params['per_device_train_batch_size']\n",
    "    log_df['per_device_eval_batch_size'] = train_params['per_device_eval_batch_size']\n",
    "    log_df['split'] = str(train_params['split'])\n",
    "    log_df['target_map'] = str(target_map)\n",
    "    log_df['binary'] = train_params['binary']\n",
    "\n",
    "    float_cols = log_df.select_dtypes(include='float64')\n",
    "    log_df[float_cols.columns] = float_cols.apply(lambda x: round(x, 3))\n",
    "    log_df.to_csv('output/training_logs.csv', mode='a', header= not os.path.isfile('output/training_logs.csv'), index=False)\n",
    "    \n",
    "def finetune(train_params, ds, target_map, save_logs):\n",
    "    params_passed = {k: train_params[k] for k in train_params if k not in ['model', 'split', 'binary']}\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    trained_model_path = f'output/models/{train_params['model']}_{timestamp}'\n",
    "    try:\n",
    "        def tokenize_fn(batch):\n",
    "            return tokenizer(batch['sentence'], truncation=True)\n",
    "        \n",
    "        #Tokenize dataset\n",
    "        tokenizer = AutoTokenizer.from_pretrained(train_params['model'], trust_remote_code=True)\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        tokenized_datasets = ds.map(tokenize_fn, batched=True)\n",
    "        \n",
    "        #Change labels\n",
    "        config = AutoConfig.from_pretrained(train_params['model'], trust_remote_code=True)\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "        config.id2label = reversed_target_map\n",
    "        config.label2id = target_map\n",
    "        \n",
    "        #Load model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            train_params['model'], config=config, ignore_mismatched_sizes=True, trust_remote_code=True)\n",
    "    \n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=f'{trained_model_path}/checkpoints',\n",
    "          evaluation_strategy='epoch',\n",
    "          logging_strategy='epoch',\n",
    "          use_cpu = False,\n",
    "          **params_passed\n",
    "        )\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        if save_logs:\n",
    "            save_logs_from_training_run(trainer, train_params, timestamp, trained_model_path, colab, target_map)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        if save_logs:\n",
    "            err_df = pd.DataFrame([[trained_model_path, str(e)]])\n",
    "            err_df.to_csv('output/error_logs.csv', mode='a', index=False)\n",
    "        \n",
    "        \n",
    "def to_binary_classification(x):\n",
    "    if x in ['Positive', 'Negative']:\n",
    "        return 'Pathos'\n",
    "    else:\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ldKE0vAo6iUF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load PolarIs-Pathos to df\n",
    "# source: https://github.com/kaatkaa/PolarIs-Corpora/blob/main/PolarIs-Full-Corpus/PolarIs-Full-Corpus-Pathos/PolarIs-Pathos.xlsx\n",
    "if colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  data_path = 'drive/MyDrive/master_lm/PolarIs-Pathos.xlsx'\n",
    "else:\n",
    "  data_path = 'data/PolarIs-Pathos.xlsx'\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "df['label'] = df[['No_pathos', 'Positive', 'Negative']].idxmax(axis=1)\n",
    "df = df.rename(columns={'Sentence':'sentence'})\n",
    "\n",
    "#run looped \n",
    "for train_params in tqdm(train_params_looped):\n",
    "    run_df = df.copy()\n",
    "    if train_params['binary']:\n",
    "        run_df['label'] = run_df['label'].apply(lambda x: to_binary_classification(x))\n",
    "    ds, target_map, reversed_target_map, = df_to_ds(run_df, train_params)\n",
    "    finetune(train_params, ds, target_map,save_logs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T10:42:22.870218Z",
     "start_time": "2024-04-08T10:32:14.277138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19eea2f230ac43638770af6a1df12b1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/14029 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c28da52aa6b4ca4bab2ac007e958c8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1559 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8e698cf4b8e40629e64011d919a02cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5262' max='5262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5262/5262 04:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.616746</td>\n",
       "      <td>0.738935</td>\n",
       "      <td>0.632855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.639965</td>\n",
       "      <td>0.758820</td>\n",
       "      <td>0.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.795359</td>\n",
       "      <td>0.748557</td>\n",
       "      <td>0.701499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakub\\AppData\\Local\\Temp\\ipykernel_24020\\3118221523.py:17: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  split = [pd.concat([i[0],i[1]], axis=0) for i in zip(split, split_t)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8910 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c04ebda8fb444a8a9deab3ed889d4a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/6678 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35817533ff964eb4bdc1aa0abfcc6de4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3342' max='3342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3342/3342 05:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.637400</td>\n",
       "      <td>0.724325</td>\n",
       "      <td>0.644954</td>\n",
       "      <td>0.503218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.497100</td>\n",
       "      <td>0.760740</td>\n",
       "      <td>0.651093</td>\n",
       "      <td>0.507485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>1.002575</td>\n",
       "      <td>0.719227</td>\n",
       "      <td>0.544640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
