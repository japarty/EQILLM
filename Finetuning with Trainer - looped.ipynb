{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7Rwla6Ky6iUA"
   },
   "source": [
    "# Finetuning with Trainer and text-classification model validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544715,
     "status": "ok",
     "timestamp": 1712223783678,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "kTnSLtc16iUE",
    "outputId": "a50dcaf4-d038-46e9-a607-73102b1b945f",
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:34.957010Z",
     "start_time": "2024-04-08T18:26:34.953715Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   colab = True\n",
    "else:\n",
    "   colab = False\n",
    "\n",
    "if colab:\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "    !pip install datasets\n",
    "    !pip install torchinfo\n",
    "    #ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F7z05bbBe5G9",
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:34.988680Z",
     "start_time": "2024-04-08T18:26:34.986134Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import numbers\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments, DataCollatorWithPadding, AutoTokenizer, pipeline\n",
    "from transformers.modelcard import parse_log_history"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare parameters for each run of finetnuning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "xmkxi8ZMdO9U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ],
   "metadata": {
    "id": "vURNs2uSdO9U",
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:35.003397Z",
     "start_time": "2024-04-08T18:26:35.000893Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# tested = 'distilbert-base-cased', 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch), \n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# model_path = 'SamLowe/roberta-base-go_emotions'\n",
    "# model_path = 'distilbert-base-uncased'\n",
    "# model_path = 'xlmoberta'\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103\n",
    "\n",
    "\n",
    "#if list of objects is provided list of all combinations of parameters will be created for running\n",
    "params_tested = {'model': ['michellejieli/emotion_text_classifier'],\n",
    "                 'num_train_epochs': 3,\n",
    "                 'save_strategy': 'epoch', # 'epoch'/'no'\n",
    "                 'per_device_train_batch_size': 8,\n",
    "                 'per_device_eval_batch_size': 64,\n",
    "                 'split': [(80, 10, 10)], #may be tuple of len 2/3, dividing respectively into 2:{train, test} and 3:{train, test, validate}. By default labels are stratified. You may pass \"balanced\" instead of the last element, number of labels for each class will be   \n",
    "                 'binary': True\n",
    "                 }\n",
    "\n",
    "save_logs = True\n",
    "\n",
    "def ratio_split_tuple(split):\n",
    "    split_s = sum([i for i in split if isinstance(i, numbers.Integral)])\n",
    "    new_split = tuple([i/split_s if isinstance(i, numbers.Integral) else i for i in split])\n",
    "    return new_split\n",
    "\n",
    "if isinstance(params_tested['split'], list):\n",
    "    params_tested['split'] = list(map(ratio_split_tuple, params_tested['split']))\n",
    "else:\n",
    "    params_tested['split'] = ratio_split_tuple(params_tested['split'])\n",
    "\n",
    "params_tested={i:[q] if type(q) is not list else q for (i,q) in params_tested.items()}\n",
    "keys = list(params_tested.keys())\n",
    "combinations = list(itertools.product(*params_tested.values()))\n",
    "result = [{keys[i]: combination[i] for i in range(len(keys))} for combination in combinations]\n",
    "print(*result, sep='\\n')\n",
    "\n",
    "train_params_looped = result"
   ],
   "metadata": {
    "id": "oylcN846dO9V",
    "outputId": "79bedc27-b958-4529-83e3-9f112e0f732b",
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:35.010872Z",
     "start_time": "2024-04-08T18:26:35.003911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'michellejieli/emotion_text_classifier', 'num_train_epochs': 3, 'save_strategy': 'epoch', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'split': (0.8, 0.1, 0.1), 'binary': True}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 5420,
     "status": "ok",
     "timestamp": 1711981638003,
     "user": {
      "displayName": "Jakub Partyka",
      "userId": "06989008944538163465"
     },
     "user_tz": -120
    },
    "id": "NmTIZtvK6iUF",
    "outputId": "9ff554fa-71dd-4258-fac5-23ee1f950e71",
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:35.021151Z",
     "start_time": "2024-04-08T18:26:35.010872Z"
    }
   },
   "source": [
    "def df_to_ds(df, train_params_instance):\n",
    "    temp_split = train_params_instance['split']\n",
    "    \n",
    "    target_map = {k:i for i,k in enumerate(df['label'].unique())}\n",
    "    reversed_target_map = {v:k for k, v in target_map.items()}\n",
    "    \n",
    "    df['label'] = df['label'].map(target_map)\n",
    "    \n",
    "    if temp_split[-1]=='balanced' or len(temp_split)==2:\n",
    "        if temp_split[-1]=='balanced':\n",
    "            split = [pd.Series(name='sentence'), pd.Series(name='sentence'), pd.Series(name='label'), pd.Series(name='label')]\n",
    "            n_for_training = df.groupby('label').count()['Source'].min()\n",
    "            train_percentage = df.groupby('label').count()['Source'].apply(lambda x: (temp_split[0]*n_for_training)/x).to_dict()\n",
    "            \n",
    "            for label_name, group in df.groupby('label'):\n",
    "                split_t = train_test_split(group['sentence'], group['label'], test_size=1-train_percentage[label_name], random_state=42, shuffle=True)\n",
    "                split = [pd.concat([i[0],i[1]], axis=0) for i in zip(split, split_t)]\n",
    "        else:\n",
    "            split = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=temp_split[1], random_state=42, shuffle=True)\n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(pd.concat([split[0], split[2]], axis=1))\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([split[1], split[3]], axis=1))\n",
    "    elif len(temp_split)==3:\n",
    "        non_train_size = 1-temp_split[0]\n",
    "        split = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=non_train_size, random_state=42, shuffle=True)\n",
    "        split2 = train_test_split(df['sentence'], df['label'], stratify=df['label'], test_size=temp_split[2]/non_train_size, random_state=42, shuffle=True)\n",
    "        ds = DatasetDict()\n",
    "        ds['train'] = Dataset.from_pandas(pd.concat([split[0], split[2]], axis=1))\n",
    "        ds['test'] = Dataset.from_pandas(pd.concat([split2[0], split2[2]], axis=1))\n",
    "        ds['validate'] = Dataset.from_pandas(pd.concat([split2[1], split2[3]], axis=1))\n",
    "    return ds, target_map, reversed_target_map\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  acc = np.mean(predictions == labels)\n",
    "  f1 = f1_score(labels, predictions, average='macro')\n",
    "  return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "def save_logs_from_training_run(trainer, train_params, timestamp, trained_model_path, colab, target_map):\n",
    "    log_history = parse_log_history(trainer.state.log_history)\n",
    "    log_df = pd.DataFrame(log_history[1])\n",
    "    log_df.insert(0, 'model', train_params['model'])\n",
    "    log_df.insert(0, 'timestamp', timestamp)\n",
    "    log_df['model_path'] = trained_model_path\n",
    "    log_df['samples_per_s'] = log_history[0]['train_samples_per_second']\n",
    "    log_df['steps_per_s'] = log_history[0]['train_steps_per_second']\n",
    "    log_df['colab'] = colab\n",
    "    log_df['per_device_train_batch_size'] = train_params['per_device_train_batch_size']\n",
    "    log_df['per_device_eval_batch_size'] = train_params['per_device_eval_batch_size']\n",
    "    log_df['split'] = str(train_params['split'])\n",
    "    log_df['target_map'] = str(target_map)\n",
    "    log_df['binary'] = train_params['binary']\n",
    "\n",
    "    float_cols = log_df.select_dtypes(include='float64')\n",
    "    log_df[float_cols.columns] = float_cols.apply(lambda x: round(x, 3))\n",
    "    log_df.to_csv('output/training_logs.csv', mode='a', header= not os.path.isfile('output/training_logs.csv'), index=False)\n",
    "    \n",
    "def finetune(train_params, ds, target_map, save_logs):\n",
    "    params_passed = {k: train_params[k] for k in train_params if k not in ['model', 'split', 'binary']}\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    trained_model_path = f'output/models/{train_params['model']}_{timestamp}'\n",
    "    try:\n",
    "        def tokenize_fn(batch):\n",
    "            return tokenizer(batch['sentence'], truncation=True)\n",
    "        \n",
    "        #Tokenize dataset\n",
    "        tokenizer = AutoTokenizer.from_pretrained(train_params['model'], trust_remote_code=True)\n",
    "        # tokenizer.pad_token = tokenizer.eos_token\n",
    "        if tokenizer.pad_token is None:\n",
    "            print('isnone')\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            # model.resize_token_embeddings(len(tokenizer))\n",
    "        tokenized_datasets = ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "        # data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        \n",
    "        #Change labels\n",
    "        config = AutoConfig.from_pretrained(train_params['model'], trust_remote_code=True)\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "        config.id2label = reversed_target_map\n",
    "        config.label2id = target_map\n",
    "        \n",
    "        #Load model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            train_params['model'], config=config, ignore_mismatched_sizes=True, trust_remote_code=True)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "        training_args = TrainingArguments(\n",
    "          output_dir=f'{trained_model_path}/checkpoints',\n",
    "          evaluation_strategy='epoch',\n",
    "          logging_strategy='epoch',\n",
    "          use_cpu = False,\n",
    "          **params_passed\n",
    "        )\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            # data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        if save_logs:\n",
    "            save_logs_from_training_run(trainer, train_params, timestamp, trained_model_path, colab, target_map)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        if save_logs:\n",
    "            err_df = pd.DataFrame([[trained_model_path, str(e)]])\n",
    "            err_df.to_csv('output/error_logs.csv', mode='a', header= not os.path.isfile('output/error_logs.csv'), index=False)\n",
    "        \n",
    "        \n",
    "def to_binary_classification(x):\n",
    "    if x in ['Positive', 'Negative']:\n",
    "        return 'Pathos'\n",
    "    else:\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ldKE0vAo6iUF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load PolarIs-Pathos to df\n",
    "# source: https://github.com/kaatkaa/PolarIs-Corpora/blob/main/PolarIs-Full-Corpus/PolarIs-Full-Corpus-Pathos/PolarIs-Pathos.xlsx\n",
    "if colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  data_path = 'drive/MyDrive/master_lm/PolarIs-Pathos.xlsx'\n",
    "else:\n",
    "  data_path = 'data/PolarIs-Pathos.xlsx'\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "df['label'] = df[['No_pathos', 'Positive', 'Negative']].idxmax(axis=1)\n",
    "df = df.rename(columns={'Sentence':'sentence'})\n",
    "\n",
    "#run looped \n",
    "for train_params in tqdm(train_params_looped):\n",
    "    run_df = df.copy()\n",
    "    if train_params['binary']:\n",
    "        run_df['label'] = run_df['label'].apply(lambda x: to_binary_classification(x))\n",
    "    ds, target_map, reversed_target_map, = df_to_ds(run_df, train_params)\n",
    "    finetune(train_params, ds, target_map,save_logs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:31:28.133003Z",
     "start_time": "2024-04-08T18:26:35.022156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f69dcd389d24761b2f790d8ff726c10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12470 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdbca1f581fb4496959dbe92ba368478"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7793 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "061ddfa768e54155afc2044ec4b34ad7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/7795 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f018e592e9104345b9f2ba61dd978156"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at michellejieli/emotion_text_classifier and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4677' max='4677' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4677/4677 04:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>0.469275</td>\n",
       "      <td>0.793276</td>\n",
       "      <td>0.738836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.330814</td>\n",
       "      <td>0.871295</td>\n",
       "      <td>0.850106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369600</td>\n",
       "      <td>0.234900</td>\n",
       "      <td>0.920441</td>\n",
       "      <td>0.906195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validate"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:19:17.901102Z",
     "start_time": "2024-04-08T18:19:17.876966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def validate(checkpoint_path, validation_ds, reversed_target_map, save=True):\n",
    "#     task = 'text-classification'\n",
    "#     classif_out_path = checkpoint_path.rsplit('/', 1)[0]\n",
    "#     classifier = pipeline(task, model=checkpoint_path, device=0)\n",
    "#     predicted = classifier(validation_ds['sentence'])\n",
    "#     # out_df = pd.DataFrame(ds['sentence', 'label'], columns=['sentence', 'label'])\n",
    "#     # out_df['predicted'] = [i['label'] for i in predicted]\n",
    "#     return predicted\n",
    "# \n",
    "# \n",
    "# print(\"acc:\", accuracy_score(validated_df['label'], validated_df['predicted']))\n",
    "# print(\"f1:\", f1_score(validated_df['label'], validated_df['predicted'], average='macro'))\n",
    "# \n",
    "# def plot_cm(cm):\n",
    "#     classes = list(target_map.keys())\n",
    "#     df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "#     ax = sn.heatmap(df_cm, annot=True, fmt='.2g')\n",
    "#     ax.set_xlabel(\"Predicted\")\n",
    "#     ax.set_ylabel(\"Target\")\n",
    "# \n",
    "# cm = confusion_matrix(validated_df['label'], validated_df['predicted'], normalize='true')\n",
    "# plot_cm(cm)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m     out_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [i[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m predicted]\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m predicted\n\u001B[1;32m---> 10\u001B[0m validated_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[43mX_val\u001B[49m, y_val], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# validated_df = ds_dict['validate'].copy(deep=True)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m validated_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [i[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m pred_val]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T18:55:26.809984Z",
     "start_time": "2024-04-08T18:55:00.780442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# checkpoint_path = 'output/models/michellejieli/emotion_text_classifier_2024-04-08_20-26/checkpoints/checkpoint-4677'\n",
    "# \n",
    "# pred = validate(checkpoint_path, ds['validate'], reversed_target_map, save=True)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput/models/michellejieli/emotion_text_classifier_2024-04-08_20-26/checkpoints/checkpoint-4677\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mds\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalidate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreversed_target_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[16], line 5\u001B[0m, in \u001B[0;36mvalidate\u001B[1;34m(checkpoint_path, validation_ds, reversed_target_map, save)\u001B[0m\n\u001B[0;32m      3\u001B[0m classif_out_path \u001B[38;5;241m=\u001B[39m checkpoint_path\u001B[38;5;241m.\u001B[39mrsplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      4\u001B[0m classifier \u001B[38;5;241m=\u001B[39m pipeline(task, model\u001B[38;5;241m=\u001B[39mcheckpoint_path, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m predicted \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_ds\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msentence\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m out_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentence\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      7\u001B[0m out_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpredicted\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [i[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m predicted]\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:155\u001B[0m, in \u001B[0;36mTextClassificationPipeline.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    122\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;124;03m    Classify the text(s) given as inputs.\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001B[39;00m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 155\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     _legacy \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_k\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1177\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[0;32m   1174\u001B[0m     final_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   1175\u001B[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[0;32m   1176\u001B[0m     )\n\u001B[1;32m-> 1177\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfinal_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[1;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m    124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator)\n\u001B[1;32m--> 125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;66;03m# Try to infer the size of the batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1102\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[1;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[0;32m   1101\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1102\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:186\u001B[0m, in \u001B[0;36mTextClassificationPipeline._forward\u001B[1;34m(self, model_inputs)\u001B[0m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(model_forward)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    185\u001B[0m     model_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 186\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1198\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m   1193\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1198\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1205\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1206\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1208\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1209\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1210\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    826\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    828\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    829\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    830\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    833\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    834\u001B[0m )\n\u001B[1;32m--> 835\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    836\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    847\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    848\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    513\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    514\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    515\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    521\u001B[0m         output_attentions,\n\u001B[0;32m    522\u001B[0m     )\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    452\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    453\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[1;32m--> 455\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    458\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[0;32m    460\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:237\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[1;32m--> 237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:467\u001B[0m, in \u001B[0;36mRobertaLayer.feed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[1;32m--> 467\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    468\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:365\u001B[0m, in \u001B[0;36mRobertaIntermediate.forward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    364\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 365\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    366\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_act_fn(hidden_states)\n\u001B[0;32m    367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
