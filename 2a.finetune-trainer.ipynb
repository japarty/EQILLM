{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set run environment (local/colab)\n",
    "# if colab install required packages and set appropriate root_path\n",
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    colab = True\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "    !pip install datasets\n",
    "    !pip install torchinfo\n",
    "    #ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_path = '/content/drive/Othercomputers/My computer/EQILLM/'\n",
    "else:\n",
    "    colab = False\n",
    "    root_path = ''\n",
    "\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import openai\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from eqillm import finetune, get_log_for_val, validate, val_metrics, yeelight_eow_notification, add_root, param_combinations, load_PolarIs, df_to_ds\n",
    "\n",
    "\n",
    "dotenv_config = dotenv_values('.env')\n",
    "yeelight_notify = dotenv_config['YEELIGHT_NOTIFY'] if 'YEELIGHT_NOTIFY' in dotenv_config else False"
   ],
   "id": "9e65fd763cce7d4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# tested = 'distilbert-base-cased', 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch),\n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# model_path = 'SamLowe/roberta-base-go_emotions'\n",
    "# model_path = 'distilbert-base-uncased'\n",
    "# model_path = 'xlmoberta'\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#if list of objects is provided list of all combinations of parameters will be created for running\n",
    "params_tested = {'model': ['distilbert-base-cased'], # Pre-trained model names from the Hugging Face hub used for fine-tuning\n",
    "                 'num_train_epochs': 10, # Number of times the model sees the entire training dataset.\n",
    "                 'save_strategy': 'epoch', # Controls when to save model checkpoints ('epoch' or 'no').\n",
    "                 'per_device_train_batch_size': 8, # Number of samples processed in each training step (personally, 8/16 work best, 16 is faster, but you may find linear drop in inference speed during fine-tuning).\n",
    "                 'per_device_eval_batch_size': 64, # Number of samples processed in each evaluation step.\n",
    "                 'split': (80, 10, 10), # Divides the dataset into training, testing, (and optionally) validation sets. Use 'balanced' for equal class representation in the validation set. Examples: (90,10) -> split into train and test proportionally; (80, 10, 10) splits into train,test, validate proportionally.\n",
    "                 'binary': [False], # Indicates whether the task is binary (two classes) or multi-class classification.,\n",
    "                 'balanced': [True], # his way labels used for training are split evenly, fitting size to the lowest label count. n (equal to 80% of least represented label) will be taken from each label, rest will be used for test.\n",
    "                 }\n",
    "\n",
    "# Controls whether to save logs during a process. When set to False, logging is disabled.\n",
    "save_logs = True\n",
    "\n",
    "train_params_looped = param_combinations(params_tested)\n",
    "print(*train_params_looped, sep='\\n')"
   ],
   "id": "d347366826e14820",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_path = os.path.join(root_path, 'data/PolarIs-Pathos.xlsx')\n",
    "\n",
    "\n",
    "#run looped\n",
    "for train_params in train_params_looped:\n",
    "    polarIs_df = load_PolarIs(data_path)\n",
    "    dataset, target_map, reversed_target_map = df_to_ds(polarIs_df, train_params['split'], train_params['binary'], train_params['balanced'])\n",
    "    finetune(dataset, train_params, target_map, reversed_target_map, save_logs, root_path, colab)\n",
    "\n",
    "if yeelight_notify:\n",
    "    yeelight_eow_notification(dotenv_config['YEELIGHT_PORT'])"
   ],
   "id": "95eed4f6c66503e2",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
