{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T22:46:30.382417Z",
     "start_time": "2024-04-29T22:46:26.396064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set run environment (local/colab), if colab move proper dir\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    colab = True\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/Othercomputers/My computer/EQILLM/\n",
    "    \n",
    "    !pip install -r requirements.txt -q --exists-action i\n",
    "    !pip install transformers[torch] -q --exists-action i\n",
    "    !pip install accelerate -U -q --exists-action i\n",
    "else:\n",
    "    colab = False\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import evaluate\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import torch, gc\n",
    "import wandb\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from huggingface_hub import login\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, BitsAndBytesConfig, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "from eqillm import finetune, yeelight_eow_notification, param_combinations, load_PolarIs, split_ds, encode_labels, init_model\n",
    "\n",
    "dotenv_config = dotenv_values('.env')\n",
    "yeelight_notify = dotenv_config['YEELIGHT_NOTIFY'] if ('YEELIGHT_NOTIFY' in dotenv_config) and (colab) else False\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "login(token=dotenv_config['HF_TOKEN'])"
   ],
   "id": "893a3345790ae7b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Jakub\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-04-29T22:46:30.385811Z",
     "start_time": "2024-04-29T22:46:30.383423Z"
    }
   },
   "source": [
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# works = [']\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch),\n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# 'xlnet-base-cased' - needs data collator\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103\n",
    "\n",
    "\n",
    "# ['michellejieli/emotion_text_classifier', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased']\n",
    "\n",
    "\n",
    "\n",
    "# to_solve\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "d347366826e14820",
    "ExecuteTime": {
     "end_time": "2024-04-29T22:50:54.400396Z",
     "start_time": "2024-04-29T22:50:54.392275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "params_tested = {'model_name': [\n",
    "                              'microsoft/phi-2',\n",
    "                            'google/gemma-2b',\n",
    "                            'mistralai/Mistral-7B-v0.1',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-irony',\n",
    "                           # 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "                           # 'michellejieli/emotion_text_classifier',\n",
    "                           # 'j-hartmann/emotion-english-distilroberta-base',\n",
    "                           # 'roberta-base',\n",
    "                           # 'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "                           # 'ProsusAI/finbert',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "                           # 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                           # 'joeddav/distilbert-base-uncased-go-emotions-student',\n",
    "                           # 'camembert-base'\n",
    "                           # 'joeddav/distilbert-base-uncased-go-emotions-student',\n",
    "                           # 'ctrl',\n",
    "                           # 'camembert-base',\n",
    "                           # 'papluca/xlm-roberta-base-language-detection',\n",
    "                           # 'mistralai/Mistral-7B-v0.1',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-irony',\n",
    "                           # 'meta-llama/Meta-Llama-3-8B',\n",
    "                           # 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                           # 'j-hartmann/emotion-english-distilroberta-base',\n",
    "                           # 'arpanghoshal/EmoRoBERTa',\n",
    "                           # 'ProsusAI/finbert',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "                           # 'michellejieli/emotion_text_classifier',\n",
    "                           # 'xlnet-base-cased',\n",
    "                           # 'roberta-base',\n",
    "                           # 'distilroberta-base',\n",
    "                           # 'flaubert/flaubert_base_cased',\n",
    "                           #  'celine98/canine-s-finetuned-sst2',\n",
    "                           #  'lytang/MiniCheck-Flan-T5-Large',\n",
    "                           #  'michelecafagna26/t5-base-finetuned-sst2-sentiment',\n",
    "                           #  'facebook/tart-full-flan-t5-xl',\n",
    "                           #  'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "                           #  'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "                           #  'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
    "                           ], # Pre-trained model names from the Hugging Face hub used for fine-tuning\n",
    "                 'num_train_epochs': 10, # Number of times the model sees the entire training dataset.\n",
    "                 'save_strategy': 'epoch', # Controls when to save model checkpoints ('epoch' or 'no').\n",
    "                 'per_device_train_batch_size': 16, # Number of samples processed in each training step (personally, 8/16 work best, 16 is faster, but you may find linear drop in inference speed during fine-tuning).\n",
    "                 'per_device_eval_batch_size': 16, # Number of samples processed in each evaluation step.\n",
    "                 'split': (0.8, 0.1, 0.1), # Divides the dataset into training, testing, (and optionally) validation sets. Use 'balanced' for equal class representation in the validation set. Examples: (90,10) -> split into train and test proportionally; (80, 10, 10) splits into train,test, validate proportionally.\n",
    "                 'binary': [True], # Indicates whether the task is binary (two classes) or multi-class classification.,\n",
    "                 'balanced': [False], # his way labels used for training are split evenly, fitting size to the lowest label count. n (equal to 80% of least represented label) will be taken from each label, rest will be used for test.\n",
    "                 'learning_rate': [\n",
    "                                    # 1e-3, \n",
    "                                    1e-4, \n",
    "                                    # 1e-5,\n",
    "                                    ],\n",
    "                 }\n",
    "\n",
    "# Controls whether to save logs during a process. When set to False, logging is disabled.\n",
    "save_logs = True\n",
    "\n",
    "train_params_looped = param_combinations(params_tested)\n",
    "print(len(train_params_looped))\n",
    "print(*train_params_looped, sep='\\n')"
   ],
   "id": "d347366826e14820",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'model_name': 'microsoft/phi-2', 'num_train_epochs': 10, 'save_strategy': 'epoch', 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'split': (0.8, 0.1, 0.1), 'binary': True, 'balanced': False, 'learning_rate': 0.0001}\n",
      "{'model_name': 'google/gemma-2b', 'num_train_epochs': 10, 'save_strategy': 'epoch', 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'split': (0.8, 0.1, 0.1), 'binary': True, 'balanced': False, 'learning_rate': 0.0001}\n",
      "{'model_name': 'mistralai/Mistral-7B-v0.1', 'num_train_epochs': 10, 'save_strategy': 'epoch', 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'split': (0.8, 0.1, 0.1), 'binary': True, 'balanced': False, 'learning_rate': 0.0001}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "95eed4f6c66503e2",
    "ExecuteTime": {
     "end_time": "2024-04-30T02:00:42.186258Z",
     "start_time": "2024-04-29T22:51:04.355086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "data_path = 'data/PolarIs-Pathos.xlsx'\n",
    "\n",
    "#run looped\n",
    "for train_params in tqdm(train_params_looped):\n",
    "    os.environ[\"WANDB_PROJECT\"]=\"pathos-recognition-binary\"\n",
    "\n",
    "    df = load_PolarIs(data_path, train_params['binary'])\n",
    "    df, target_map = encode_labels(df)\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = split_ds(ds, train_size=0.8, val_size=0.1)\n",
    "\n",
    "    finetune(ds, train_params, target_map, colab)\n",
    "\n",
    "if yeelight_notify:\n",
    "    yeelight_eow_notification(dotenv_config['YEELIGHT_PORT'])"
   ],
   "id": "95eed4f6c66503e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32cc732fb2b74011b57ff4979432ec4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12470 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6945f21c31d14494a80f8451d97c163a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1558 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e63cff548544e54872d1cab29db4a5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1560 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "648a4fa84ad84c9eb89337f402dc1e19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09a4eeb9d43a4580a9fc1ff117fe9e88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared\n",
      "Model perfed\n",
      "trainable params: 2,298,880 || all params: 2,650,864,640 || trainable%: 0.0867218931254068\n",
      "cuda:0\n",
      "Model to cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7800/7800 1:05:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.705200</td>\n",
       "      <td>0.636493</td>\n",
       "      <td>0.479491</td>\n",
       "      <td>0.686235</td>\n",
       "      <td>0.564530</td>\n",
       "      <td>0.664313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.627146</td>\n",
       "      <td>0.497592</td>\n",
       "      <td>0.627530</td>\n",
       "      <td>0.555058</td>\n",
       "      <td>0.681001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.583900</td>\n",
       "      <td>0.627561</td>\n",
       "      <td>0.486172</td>\n",
       "      <td>0.676113</td>\n",
       "      <td>0.565622</td>\n",
       "      <td>0.670732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>0.628731</td>\n",
       "      <td>0.530142</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.704750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>0.627373</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.688259</td>\n",
       "      <td>0.581694</td>\n",
       "      <td>0.686136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.653796</td>\n",
       "      <td>0.572104</td>\n",
       "      <td>0.489879</td>\n",
       "      <td>0.527808</td>\n",
       "      <td>0.722080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.625058</td>\n",
       "      <td>0.520868</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.570906</td>\n",
       "      <td>0.698973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.640612</td>\n",
       "      <td>0.528626</td>\n",
       "      <td>0.560729</td>\n",
       "      <td>0.544204</td>\n",
       "      <td>0.702182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.630436</td>\n",
       "      <td>0.493469</td>\n",
       "      <td>0.688259</td>\n",
       "      <td>0.574810</td>\n",
       "      <td>0.677150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>0.627328</td>\n",
       "      <td>0.531624</td>\n",
       "      <td>0.629555</td>\n",
       "      <td>0.576460</td>\n",
       "      <td>0.706675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.032 MB of 0.067 MB uploaded\\r'), FloatProgress(value=0.47331156993259, max=1.0))…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef88eb0cd508466a85618a50c68b41d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▁▃▂▆▄█▅▆▃▆</td></tr><tr><td>eval/f1-score</td><td>▁▇▇▇▇█▆█▇██</td></tr><tr><td>eval/loss</td><td>█▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>eval/precision</td><td>█▁▂▁▃▂▅▃▃▂▃</td></tr><tr><td>eval/recall</td><td>▁█▇█▇█▅▇▆█▇</td></tr><tr><td>eval/runtime</td><td>█▁▅▆▃▆▆▅▃▃▅</td></tr><tr><td>eval/samples_per_second</td><td>▁█▃▃▆▃▃▄▆▆▄</td></tr><tr><td>eval/steps_per_second</td><td>▁█▃▃▆▃▃▄▆▆▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▅▅▁▇▄▁▆▁▄▄▅▃█▇</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▃▃▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70668</td></tr><tr><td>eval/f1-score</td><td>0.57646</td></tr><tr><td>eval/loss</td><td>0.62733</td></tr><tr><td>eval/precision</td><td>0.53162</td></tr><tr><td>eval/recall</td><td>0.62955</td></tr><tr><td>eval/runtime</td><td>46.6817</td></tr><tr><td>eval/samples_per_second</td><td>33.375</td></tr><tr><td>eval/steps_per_second</td><td>2.099</td></tr><tr><td>total_flos</td><td>9.007685183195136e+16</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>7800</td></tr><tr><td>train/grad_norm</td><td>12.75031</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.5246</td></tr><tr><td>train_loss</td><td>0.56781</td></tr><tr><td>train_runtime</td><td>3947.0573</td></tr><tr><td>train_samples_per_second</td><td>31.593</td></tr><tr><td>train_steps_per_second</td><td>1.976</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/phi-2</strong> at: <a href='https://wandb.ai/jpartyka/pathos-recognition-binary/runs/6456p9r1' target=\"_blank\">https://wandb.ai/jpartyka/pathos-recognition-binary/runs/6456p9r1</a><br/> View project at: <a href='https://wandb.ai/jpartyka/pathos-recognition-binary' target=\"_blank\">https://wandb.ai/jpartyka/pathos-recognition-binary</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240430_004730-6456p9r1\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12470 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b790b35dad54c1ea0448824c482f968"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1558 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3589b6d41a2c4843bd799bb0b5751de8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1560 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf2ac3a3ced3400dbd992af6dff40ff0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b3393e9235d41c1a454df562e7f0ef7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-2b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prepared\n",
      "Model perfed\n",
      "trainable params: 234,496 || all params: 2,506,411,008 || trainable%: 0.009355847833876095\n",
      "cuda:0\n",
      "Model to cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\wandb\\run-20240430_015708-ngcvy7af</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jpartyka/pathos-recognition-binary/runs/ngcvy7af' target=\"_blank\">google/gemma-2b</a></strong> to <a href='https://wandb.ai/jpartyka/pathos-recognition-binary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/jpartyka/pathos-recognition-binary' target=\"_blank\">https://wandb.ai/jpartyka/pathos-recognition-binary</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/jpartyka/pathos-recognition-binary/runs/ngcvy7af' target=\"_blank\">https://wandb.ai/jpartyka/pathos-recognition-binary/runs/ngcvy7af</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7800/7800 2:03:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.598248</td>\n",
       "      <td>0.566735</td>\n",
       "      <td>0.558704</td>\n",
       "      <td>0.562691</td>\n",
       "      <td>0.724647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.596088</td>\n",
       "      <td>0.508499</td>\n",
       "      <td>0.726721</td>\n",
       "      <td>0.598333</td>\n",
       "      <td>0.690629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.618189</td>\n",
       "      <td>0.474138</td>\n",
       "      <td>0.779352</td>\n",
       "      <td>0.589587</td>\n",
       "      <td>0.655969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>0.545769</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>0.589003</td>\n",
       "      <td>0.716945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.770778</td>\n",
       "      <td>0.550201</td>\n",
       "      <td>0.554656</td>\n",
       "      <td>0.552419</td>\n",
       "      <td>0.715019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>1.298458</td>\n",
       "      <td>0.577215</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.512936</td>\n",
       "      <td>0.722080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1.336848</td>\n",
       "      <td>0.505068</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.550645</td>\n",
       "      <td>0.686778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>2.207862</td>\n",
       "      <td>0.564286</td>\n",
       "      <td>0.479757</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.717587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>2.832149</td>\n",
       "      <td>0.559242</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>0.515284</td>\n",
       "      <td>0.715019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>3.541602</td>\n",
       "      <td>0.504604</td>\n",
       "      <td>0.554656</td>\n",
       "      <td>0.528447</td>\n",
       "      <td>0.686136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.032 MB of 0.053 MB uploaded (0.004 MB deduped)\\r'), FloatProgress(value=0.604796…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1f55398cfe0440791bb730c66c6f0b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B sync reduced upload amount by 8.4%             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▅▁▇▇█▄▇▇▄</td></tr><tr><td>eval/f1-score</td><td>▅█▇▇▄▁▄▁▁▂</td></tr><tr><td>eval/loss</td><td>▁▁▁▁▁▃▃▅▆█</td></tr><tr><td>eval/precision</td><td>▇▃▁▆▆█▃▇▇▃</td></tr><tr><td>eval/recall</td><td>▃▇█▅▃▁▄▁▁▃</td></tr><tr><td>eval/runtime</td><td>▁▅▄▄▄▄██▇▇</td></tr><tr><td>eval/samples_per_second</td><td>█▄▅▅▅▅▁▁▂▂</td></tr><tr><td>eval/steps_per_second</td><td>█▄▅▅▅▅▁▁▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▂▁▂▂▁▂▂▁▆▂█▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▅▄▄▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.68614</td></tr><tr><td>eval/f1-score</td><td>0.52845</td></tr><tr><td>eval/loss</td><td>3.5416</td></tr><tr><td>eval/precision</td><td>0.5046</td></tr><tr><td>eval/recall</td><td>0.55466</td></tr><tr><td>eval/runtime</td><td>35.0134</td></tr><tr><td>eval/samples_per_second</td><td>44.497</td></tr><tr><td>eval/steps_per_second</td><td>2.799</td></tr><tr><td>total_flos</td><td>7.004989208604672e+16</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>7800</td></tr><tr><td>train/grad_norm</td><td>3.01516</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.1033</td></tr><tr><td>train_loss</td><td>0.32379</td></tr><tr><td>train_runtime</td><td>7404.2711</td></tr><tr><td>train_samples_per_second</td><td>16.842</td></tr><tr><td>train_steps_per_second</td><td>1.053</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">google/gemma-2b</strong> at: <a href='https://wandb.ai/jpartyka/pathos-recognition-binary/runs/ngcvy7af' target=\"_blank\">https://wandb.ai/jpartyka/pathos-recognition-binary/runs/ngcvy7af</a><br/> View project at: <a href='https://wandb.ai/jpartyka/pathos-recognition-binary' target=\"_blank\">https://wandb.ai/jpartyka/pathos-recognition-binary</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240430_015708-ngcvy7af\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12470 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c53c1a86e60407ea71340856f4bdf84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1558 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9327c327d893436596146c43dd01e5cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1560 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3c6c8903dc648a7b774502d52be769b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1363: UserWarning: Current model requires 134218752.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m     ds \u001B[38;5;241m=\u001B[39m Dataset\u001B[38;5;241m.\u001B[39mfrom_pandas(df)\n\u001B[0;32m     12\u001B[0m     ds \u001B[38;5;241m=\u001B[39m split_ds(ds, train_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, val_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m     \u001B[43mfinetune\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m yeelight_notify:\n\u001B[0;32m     17\u001B[0m     yeelight_eow_notification(dotenv_config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYEELIGHT_PORT\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\eqillm.py:235\u001B[0m, in \u001B[0;36mfinetune\u001B[1;34m(ds, params, target_map, colab)\u001B[0m\n\u001B[0;32m    232\u001B[0m timestamp \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    233\u001B[0m trained_model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput/models/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimestamp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 235\u001B[0m model, tokenizer, tokenized_datasets \u001B[38;5;241m=\u001B[39m \u001B[43minit_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_name\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorWithPadding(tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    240\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m    241\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrained_model_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/checkpoints\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    242\u001B[0m     lr_scheduler_type\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstant\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    252\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams_passed\n\u001B[0;32m    253\u001B[0m )\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\eqillm.py:320\u001B[0m, in \u001B[0;36minit_model\u001B[1;34m(model_checkpoint, ds, target_map, bnb, peft)\u001B[0m\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28mprint\u001B[39m(compute_dtype)\n\u001B[0;32m    313\u001B[0m     bnb_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(\n\u001B[0;32m    314\u001B[0m         load_in_4bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    315\u001B[0m         bnb_4bit_quant_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnf4\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    316\u001B[0m         bnb_4bit_compute_dtype\u001B[38;5;241m=\u001B[39mcompute_dtype,\n\u001B[0;32m    317\u001B[0m         bnb_4bit_use_double_quant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    318\u001B[0m     )\n\u001B[1;32m--> 320\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForSequenceClassification\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[38;5;66;43;03m# config=config,\u001B[39;49;00m\n\u001B[0;32m    323\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbnb_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m                                                               \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m peft \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    332\u001B[0m     peft_config \u001B[38;5;241m=\u001B[39m LoraConfig(\n\u001B[0;32m    333\u001B[0m         task_type\u001B[38;5;241m=\u001B[39mTaskType\u001B[38;5;241m.\u001B[39mSEQ_CLS,\n\u001B[0;32m    334\u001B[0m         r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    342\u001B[0m         \u001B[38;5;66;03m# ]\u001B[39;00m\n\u001B[0;32m    343\u001B[0m     )\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    562\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    567\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m )\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3627\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3624\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m infer_auto_device_map(model, dtype\u001B[38;5;241m=\u001B[39mtarget_dtype, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_map_kwargs)\n\u001B[0;32m   3626\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 3627\u001B[0m         \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3629\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3630\u001B[0m     model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[1;32m~\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:86\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     82\u001B[0m     device_map_without_lm_head \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     83\u001B[0m         key: device_map[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m device_map\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodules_to_not_convert\n\u001B[0;32m     84\u001B[0m     }\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisk\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m---> 86\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     87\u001B[0m \u001B[38;5;250m            \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;124;03m            Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\u001B[39;00m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;124;03m            quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\u001B[39;00m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;124;03m            in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;124;03m            `from_pretrained`. Check\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03m            https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;124;03m            for more details.\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;124;03m            \"\"\"\u001B[39;00m\n\u001B[0;32m     95\u001B[0m         )\n\u001B[0;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbitsandbytes\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m<\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.39.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m     98\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m     99\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    100\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    101\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: \n                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n                    in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to\n                    `from_pretrained`. Check\n                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                    for more details.\n                    "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "57c706680b93d58d"
   },
   "cell_type": "code",
   "source": [
    "# import datasets\n",
    "# from transformers import pipeline\n",
    "# from transformers.pipelines.pt_utils import KeyDataset\n",
    "# from tqdm.auto import tqdm\n",
    "#\n",
    "# pipe = pipeline(\"text-classification\", model=\"output/models/google/gemma-2b_2024-04-27_00-13/checkpoints/checkpoint-140290\", device=0)\n",
    "# valds = df_to_ds(polarIs_df)[0]['validate']\n",
    "# predicted = [i['label'] for i in pipe(valds['text'])]"
   ],
   "id": "57c706680b93d58d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
