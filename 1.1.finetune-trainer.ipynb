{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:59:01.693276Z",
     "start_time": "2024-04-22T17:58:59.168844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set run environment (local/colab)\n",
    "# if colab install required packages and set appropriate root_path\n",
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    colab = True\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_path = '/content/drive/Othercomputers/My computer/EQILLM/'\n",
    "    !pip install -r '/content/drive/Othercomputers/My computer/EQILLM/requirements.txt'\n",
    "    !pip install transformers[torch]\n",
    "    !pip install accelerate -U\n",
    "    !pip install datasets\n",
    "    !pip install torchinfo\n",
    "    import sys\n",
    "    sys.path.append(root_path)\n",
    "    #ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U\n",
    "else:\n",
    "    colab = False\n",
    "    root_path = ''\n",
    "\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import openai\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from eqillm import finetune, get_log_for_val, validate, val_metrics, yeelight_eow_notification, param_combinations, load_PolarIs, df_to_ds\n",
    "\n",
    "\n",
    "dotenv_config = dotenv_values(os.path.join(root_path, '.env'))\n",
    "yeelight_notify = dotenv_config['YEELIGHT_NOTIFY'] if 'YEELIGHT_NOTIFY' in dotenv_config else False"
   ],
   "id": "9e65fd763cce7d4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T17:59:01.696421Z",
     "start_time": "2024-04-22T17:59:01.694284Z"
    }
   },
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# models = ['albert', 'bart', 'bert', 'big_bird', 'bigbird_pegasus', 'biogpt', 'bloom', 'camembert', 'canine', 'code_llama', 'convbert', 'ctrl', 'data2vec-text', 'deberta', 'deberta-v2', 'distilbert', 'electra', 'ernie', 'ernie_m', 'esm', 'falcon', 'flaubert', 'fnet', 'funnel', 'gemma', 'gpt-sw3', 'gpt2', 'gpt_bigcode', 'gpt_neo', 'gpt_neox', 'gptj', 'ibert', 'layoutlm', 'layoutlmv2', 'layoutlmv3', 'led', 'lilt', 'llama', 'longformer', 'luke', 'markuplm', 'mbart', 'mega', 'megatron-bert', 'mistral', 'mixtral', 'mobilebert', 'mpnet', 'mpt', 'mra', 'mt5', 'mvp', 'nezha', 'nystromformer', 'open-llama', 'openai-gpt', 'opt', 'perceiver', 'persimmon', 'phi', 'plbart', 'qdqbert', 'qwen2', 'reformer', 'rembert', 'roberta', 'roberta-prelayernorm', 'roc_bert', 'roformer', 'squeezebert', 'stablelm', 'starcoder2', 't5', 'tapas', 'transfo-xl', 'umt5', 'xlm', 'xlm-roberta', 'xlm-roberta-xl', 'xlnet', 'xmod', 'yoso',]\n",
    "# =============================================\n",
    "# works = [']\n",
    "# ----------\n",
    "#too weak pc (cannot be loaded or runs at speeds <1 it/s) = 'albert-xlarge-v1', 't5-11B', 'LongformerForSequenceClassification', 'xlm-roberta-base', 'allenai/longformer-base-4096', 'facebook/bart-large',  flaubert/flaubert_large_cased (stopped at 0.5 epoch),\n",
    "# ----------\n",
    "# need changes to run (check error logs) =  ProsusAI/finbert (despite adding padding proposed in error)\n",
    "# =============================================\n",
    "# 'xlnet-base-cased' - needs data collator\n",
    "\n",
    "# , 'camembert-base', 'ctrl', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased'\n",
    "\n",
    "# deprecated: transfo-xl-wt103\n",
    "\n",
    "\n",
    "# ['michellejieli/emotion_text_classifier', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased']\n",
    "\n",
    "\n",
    "\n",
    "# to_solve\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T17:59:01.705514Z",
     "start_time": "2024-04-22T17:59:01.697432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#if list of objects is provided list of all combinations of parameters will be created for running\n",
    "#['michellejieli/emotion_text_classifier', 'xlnet-base-cased', 'roberta-base', 'distilroberta-base', 'flaubert/flaubert_base_cased']\n",
    "\n",
    "params_tested = {'model': [\n",
    "                            # 'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-irony',\n",
    "                           # 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "                           # 'michellejieli/emotion_text_classifier',\n",
    "                           # 'j-hartmann/emotion-english-distilroberta-base',\n",
    "                           # 'distilroberta-base',\n",
    "                           # 'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "                           # 'ProsusAI/finbert',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "                           # 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                           # 'joeddav/distilbert-base-uncased-go-emotions-student',\n",
    "                           # 'camembert-base'\n",
    "                           # 'joeddav/distilbert-base-uncased-go-emotions-student',\n",
    "                           # 'ctrl',\n",
    "                           # 'camembert-base',\n",
    "                           # 'papluca/xlm-roberta-base-language-detection',\n",
    "                           # 'mistralai/Mistral-7B-v0.1',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-irony',\n",
    "                           # 'meta-llama/Meta-Llama-3-8B',\n",
    "                           # 'distilbert/distilbert-base-uncased-finetuned-sst-2-english', \n",
    "                           # 'j-hartmann/emotion-english-distilroberta-base', \n",
    "                           # 'arpanghoshal/EmoRoBERTa', \n",
    "                           # 'ProsusAI/finbert', \n",
    "                           # 'cardiffnlp/twitter-roberta-base-sentiment-latest', \n",
    "                           # 'michellejieli/emotion_text_classifier', \n",
    "                           # 'xlnet-base-cased', \n",
    "                           # 'roberta-base', \n",
    "                           # 'distilroberta-base', \n",
    "                           # 'flaubert/flaubert_base_cased',\n",
    "                           #  'celine98/canine-s-finetuned-sst2',\n",
    "                           #  'lytang/MiniCheck-Flan-T5-Large',\n",
    "                           #  'michelecafagna26/t5-base-finetuned-sst2-sentiment',\n",
    "                           #  'facebook/tart-full-flan-t5-xl',\n",
    "                           #  'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "                           #  'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "                           #  'cardiffnlp/twitter-xlm-roberta-base-sentiment'\n",
    "                           ], # Pre-trained model names from the Hugging Face hub used for fine-tuning\n",
    "                 'num_train_epochs': 10, # Number of times the model sees the entire training dataset.\n",
    "                 'save_strategy': 'no', # Controls when to save model checkpoints ('epoch' or 'no').\n",
    "                 'per_device_train_batch_size': 8, # Number of samples processed in each training step (personally, 8/16 work best, 16 is faster, but you may find linear drop in inference speed during fine-tuning).\n",
    "                 'per_device_eval_batch_size': 64, # Number of samples processed in each evaluation step.\n",
    "                 'split': (0.6, 0.2, 0.2), # Divides the dataset into training, testing, (and optionally) validation sets. Use 'balanced' for equal class representation in the validation set. Examples: (90,10) -> split into train and test proportionally; (80, 10, 10) splits into train,test, validate proportionally.\n",
    "                 'binary': [True], # Indicates whether the task is binary (two classes) or multi-class classification.,\n",
    "                 'balanced': [False], # his way labels used for training are split evenly, fitting size to the lowest label count. n (equal to 80% of least represented label) will be taken from each label, rest will be used for test.\n",
    "                 }\n",
    "\n",
    "# Controls whether to save logs during a process. When set to False, logging is disabled.\n",
    "save_logs = True\n",
    "\n",
    "train_params_looped = param_combinations(params_tested)\n",
    "print(len(train_params_looped))\n",
    "print(*train_params_looped, sep='\\n')"
   ],
   "id": "d347366826e14820",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'num_train_epochs': 10, 'save_strategy': 'no', 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64, 'split': (0.6, 0.2, 0.2), 'binary': True, 'balanced': False}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T18:02:21.932503Z",
     "start_time": "2024-04-22T17:59:01.705514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = os.path.join(root_path, 'data/PolarIs-Pathos.xlsx')\n",
    "\n",
    "\n",
    "#run looped\n",
    "for train_params in tqdm_notebook(train_params_looped):\n",
    "    polarIs_df = load_PolarIs(data_path)\n",
    "    dataset, target_map, reversed_target_map = df_to_ds(polarIs_df, train_params['split'], train_params['binary'], train_params['balanced'])\n",
    "    finetune(dataset, train_params, target_map, reversed_target_map, save_logs, root_path, colab)\n",
    "\n",
    "if yeelight_notify:\n",
    "    yeelight_eow_notification(dotenv_config['YEELIGHT_PORT'])"
   ],
   "id": "95eed4f6c66503e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a478f6fab4b4b86bb0e1a6b05e0bd34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12470 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c75c1708fa842c492abcc1bfb20ed2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1559 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a27aa97f4e45444bbf43194940633222"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1559 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9411a229532f48b39db428732941e6ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "520b28cad82147ecbd549fbedec3f893"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b78b359bce9465aa04d2addaed7cdb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jakub\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3dcc5dc37e8425ca555d0fae14504f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d7d832617f14dbc87549da345b8f13a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79e2446051a049ac87167ecaa0c9c8a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Jakub\\DataspellProjects\\EQILLM\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
